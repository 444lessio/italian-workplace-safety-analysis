{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Library Imports ---\n",
    "# It's a best practice to group imports for clarity.\n",
    "\n",
    "# Third-party libraries for data manipulation and analysis\n",
    "import pandas as pd # type: ignore\n",
    "import plotly.express as px  # type: ignore # Good to have visualization library here\n",
    "\n",
    "# Third-party libraries for RDF and Knowledge Graph creation\n",
    "from rdflib import Graph, Literal, Namespace, URIRef # type: ignore\n",
    "from rdflib.namespace import RDF, RDFS, XSD # type: ignore\n",
    "\n",
    "# Python standard library for URL handling\n",
    "import urllib.parse\n",
    "import re # Imported to create a more robust slugify function\n",
    "\n",
    "# --- 2. Constants and Namespace Definitions ---\n",
    "# Defining namespaces here keeps the code clean and easy to manage.\n",
    "# Replace 'example.org' with a more descriptive base URI.\n",
    "BASE_NS = Namespace(\"http://workplace-safety.example.org/ontology#\")\n",
    "RESOURCE_NS = Namespace(\"http://workplace-safety.example.org/resource/\")\n",
    "\n",
    "# --- 3. Utility Functions ---\n",
    "# A well-documented utility function is a sign of a professional developer.\n",
    "\n",
    "def slugify(text_to_slugify: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and prepares a string to be used safely in a URL or as a resource name.\n",
    "\n",
    "    This function removes special characters, replaces spaces with underscores,\n",
    "    and converts the string to lowercase. It's more robust than simple replace() calls.\n",
    "\n",
    "    Args:\n",
    "        text_to_slugify: The input string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        A URL-safe \"slug\" version of the input string.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string to avoid errors\n",
    "    if not isinstance(text_to_slugify, str):\n",
    "        text_to_slugify = str(text_to_slugify)\n",
    "    \n",
    "    # Remove special characters using regular expressions\n",
    "    text_to_slugify = re.sub(r'[^\\w\\s-]', '', text_to_slugify)\n",
    "    \n",
    "    # Replace spaces and hyphens with a single underscore, and convert to lowercase\n",
    "    return re.sub(r'[-\\s]+', '_', text_to_slugify).lower()\n",
    "\n",
    "\n",
    "def create_uri(namespace: Namespace, resource_name: str) -> URIRef:\n",
    "    \"\"\"\n",
    "    Creates a full, URL-encoded URI for an RDF resource.\n",
    "\n",
    "    Args:\n",
    "        namespace: The RDFLib Namespace to use (e.g., for classes or resources).\n",
    "        resource_name: The name of the resource (e.g., \"Catania\", \"Construction_Industry\").\n",
    "\n",
    "    Returns:\n",
    "        A valid URIRef object for the resource.\n",
    "    \"\"\"\n",
    "    # First, create a clean, readable \"slug\" from the resource name\n",
    "    slug = slugify(resource_name)\n",
    "    \n",
    "    # Then, URL-encode the slug to ensure it's a valid URI component\n",
    "    # This is safer than relying on a custom replacement function.\n",
    "    encoded_slug = urllib.parse.quote(slug)\n",
    "    \n",
    "    return namespace[encoded_slug]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workplace incidents data loaded successfully. First 5 rows:\n",
      "  DataRilevazione DataProtocollo DataAccadimento DataDefinizione DataMorte  \\\n",
      "0      31/10/2022     18/11/2020      18/11/2020      25/11/2020       NaN   \n",
      "1      31/10/2022     05/06/2019      24/05/2019      29/11/2019       NaN   \n",
      "2      31/10/2022     20/07/2021      16/07/2021      01/11/2021       NaN   \n",
      "3      31/10/2022     25/11/2019      20/11/2019      15/12/2019       NaN   \n",
      "4      31/10/2022     12/05/2017      05/05/2017      20/02/2019       NaN   \n",
      "\n",
      "   LuogoAccadimento  IdentificativoInfortunato Genere  Eta LuogoNascita  ...  \\\n",
      "0                88                   33031784      M   22         Z343  ...   \n",
      "1                87                   18901323      M   55         ITAL  ...   \n",
      "2                86                   28216801      M   53         ITAL  ...   \n",
      "3                83                    5731062      M   58         ITAL  ...   \n",
      "4                87                   11404309      M   62         ITAL  ...   \n",
      "\n",
      "  Indennizzo DecisioneIstruttoriaEsitoMortale  GradoMenomazione  \\\n",
      "0         TE                               ND                -1   \n",
      "1         NE                               ND                -1   \n",
      "2         NE                               ND                -1   \n",
      "3         TE                               ND                 2   \n",
      "4         NE                               ND                -1   \n",
      "\n",
      "  GiorniIndennizzati IdentificativoDatoreLavoro  \\\n",
      "0                  4                    4602124   \n",
      "1                  0                    8012505   \n",
      "2                  0                         -1   \n",
      "3                 50                    3191593   \n",
      "4                  0                         -1   \n",
      "\n",
      "  PosizioneAssicurativaTerritoriale SettoreAttivitaEconomica  Gestione  \\\n",
      "0                           5486865                     C 15         I   \n",
      "1                          10064989                     Q 86         I   \n",
      "2                                -1                       ND         I   \n",
      "3                           6807083                     J 58         I   \n",
      "4                                -1                       ND         I   \n",
      "\n",
      "   GestioneTariffaria  GrandeGruppoTariffario  \n",
      "0                   1                       8  \n",
      "1                   4                       0  \n",
      "2                  ND                      ND  \n",
      "3                   1                       2  \n",
      "4                  ND                      ND  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os # Imported to handle file paths robustly\n",
    "\n",
    "# --- 1. Setup Project Paths ---\n",
    "# This approach ensures the code runs on any computer without modification.\n",
    "# The 'data' folder should be in the same directory as the Jupyter Notebook.\n",
    "DATA_FOLDER = 'data'\n",
    "\n",
    "# --- 2. Load the Main Workplace Incidents Dataset ---\n",
    "# Renamed variables to English for clarity and international standards.\n",
    "# Added error handling to provide a clear message if a file is missing.\n",
    "try:\n",
    "    incidents_path = os.path.join(DATA_FOLDER, 'DatiConCadenzaSemestraleInfortuniSicilia.csv')\n",
    "    workplace_incidents = pd.read_csv(incidents_path, sep=\";\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {incidents_path} was not found. Make sure it's inside the '{DATA_FOLDER}' folder.\")\n",
    "\n",
    "# --- 3. Load Typological and Enrichment Datasets ---\n",
    "# These are \"lookup tables\" used to add meaning to the main dataset.\n",
    "# Using descriptive names makes the code's purpose much clearer.\n",
    "\n",
    "# Province lookup table\n",
    "provinces = pd.read_csv(os.path.join(DATA_FOLDER, 'Provincia.csv'), sep=\";\")\n",
    "\n",
    "# Nation Codes lookup table (loaded directly from Excel)\n",
    "# This avoids creating a temporary .csv file on your local machine, which is a major improvement.\n",
    "nation_codes = pd.read_excel(os.path.join(DATA_FOLDER, 'CodiciNazioni.xls'))\n",
    "\n",
    "# Administrative Definitions lookup table\n",
    "admin_definitions = pd.read_csv(os.path.join(DATA_FOLDER, 'DefinizioneAmministrativa.csv'), sep=\";\")\n",
    "\n",
    "# Compensation Type lookup table\n",
    "compensation_types = pd.read_csv(os.path.join(DATA_FOLDER, 'TipologiaIndennizzo.csv'), sep=\";\")\n",
    "\n",
    "# Fatal Outcome Decisions lookup table\n",
    "# Specified 'latin-1' encoding as in the original code.\n",
    "fatal_outcomes = pd.read_csv(os.path.join(DATA_FOLDER, 'DecisioneIstruttoriaEsitoMortale.csv'), sep=\";\", encoding='latin-1')\n",
    "\n",
    "# Birthplace lookup table (assuming this is also a lookup table)\n",
    "birthplaces = pd.read_csv(os.path.join(DATA_FOLDER, 'LuogoNascita.csv'), sep=\";\")\n",
    "\n",
    "# --- 4. Initial Data Preview (Good Practice) ---\n",
    "# It's always a good idea to display the first few rows of the main dataframe\n",
    "# to confirm it has been loaded correctly.\n",
    "print(\"Workplace incidents data loaded successfully. First 5 rows:\")\n",
    "print(workplace_incidents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame cleaned successfully. Columns removed.\n"
     ]
    }
   ],
   "source": [
    "def clean_incidents_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the raw workplace incidents DataFrame by removing unnecessary columns.\n",
    "\n",
    "    This function takes the raw incidents DataFrame and drops a predefined list of\n",
    "    columns that are not relevant for the analysis (e.g., identifiers, internal\n",
    "    protocol dates).\n",
    "\n",
    "    Args:\n",
    "        df: The raw pandas DataFrame of workplace incidents.\n",
    "\n",
    "    Returns:\n",
    "        A new, cleaned pandas DataFrame with the specified columns removed.\n",
    "    \"\"\"\n",
    "    # Define all columns to be dropped in a single, easy-to-read list.\n",
    "    # This makes the code much more maintainable.\n",
    "    columns_to_drop = [\n",
    "        'DataRilevazione',\n",
    "        'DataProtocollo',\n",
    "        'DataDefinizione',\n",
    "        'IdentificativoInfortunato',\n",
    "        'GradoMenomazione',\n",
    "        'IdentificativoDatoreLavoro',\n",
    "        'PosizioneAssicurativaTerritoriale',\n",
    "        'Gestione',\n",
    "        'GestioneTariffaria',\n",
    "        'GrandeGruppoTariffario',\n",
    "        'DefinizioneAmministrativaEsitoMortale',\n",
    "        'SettoreAttivitaEconomica',\n",
    "        'ModalitaAccadimento'\n",
    "    ]\n",
    "    \n",
    "    # Drop all columns in a single, efficient operation.\n",
    "    # The .copy() is used to ensure the original DataFrame is not modified.\n",
    "    cleaned_df = df.drop(columns=columns_to_drop).copy()\n",
    "    \n",
    "    # The redundant pd.concat line has been removed.\n",
    "    \n",
    "    print(\"DataFrame cleaned successfully. Columns removed.\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Call the function with the new, professional naming convention\n",
    "cleaned_workplace_incidents = clean_incidents_dataframe(workplace_incidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date processing complete. 'DataAccadimento' converted, 'year' column added, and DataFrame sorted.\n"
     ]
    }
   ],
   "source": [
    "def process_and_sort_by_date(df: pd.DataFrame, date_column: str, sort_keys: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes date columns, extracts features, and sorts the DataFrame.\n",
    "\n",
    "    This function converts the specified date column to a proper datetime format,\n",
    "    creates a new 'year' column for easy analysis, and then sorts the DataFrame\n",
    "    based on the provided keys. This approach is non-destructive, preserving\n",
    "    the original full date information.\n",
    "\n",
    "    Args:\n",
    "        df: The input pandas DataFrame.\n",
    "        date_column: The name of the column containing date strings.\n",
    "        sort_keys: A list of column names to sort the DataFrame by.\n",
    "\n",
    "    Returns:\n",
    "        A new, processed, and sorted pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # It's a best practice to work on a copy to avoid unexpected side effects\n",
    "    # on the original DataFrame (this prevents the SettingWithCopyWarning).\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # --- 1. Convert Date Column to Datetime Objects (The Right Way) ---\n",
    "    # We convert the column but keep all the rich date information.\n",
    "    processed_df[date_column] = pd.to_datetime(processed_df[date_column], format='%d/%m/%Y')\n",
    "\n",
    "    # --- 2. Feature Engineering: Extract the Year into a NEW Column ---\n",
    "    # This is the key improvement. We preserve the original datetime column\n",
    "    # and create a new, separate column just for the year.\n",
    "    processed_df['year'] = processed_df[date_column].dt.year\n",
    "    \n",
    "    # Now you could easily add more features if needed:\n",
    "    # processed_df['month'] = processed_df[date_column].dt.month\n",
    "    # processed_df['weekday'] = processed_df[date_column].dt.day_name()\n",
    "\n",
    "    # --- 3. Sort the DataFrame ---\n",
    "    # We use the full list of sort keys provided.\n",
    "    processed_df = processed_df.sort_values(by=sort_keys)\n",
    "    \n",
    "    print(f\"Date processing complete. '{date_column}' converted, 'year' column added, and DataFrame sorted.\")\n",
    "    return processed_df\n",
    "\n",
    "# --- How to use the function ---\n",
    "# Define the columns to sort by. Note that we can now sort by the new 'year' column.\n",
    "# Let's assume you have an English column name 'location' instead of 'LuogoAccadimento'\n",
    "# and the date column is 'incident_date'.\n",
    "# For now, I will use your original names.\n",
    "sort_order = ['LuogoAccadimento', 'DataAccadimento'] \n",
    "sorted_incidents = process_and_sort_by_date(\n",
    "    df=cleaned_workplace_incidents, \n",
    "    date_column='DataAccadimento', \n",
    "    sort_keys=sort_order\n",
    ")\n",
    "\n",
    "# Display the result to confirm\n",
    "# print(sorted_incidents[['DataAccadimento', 'year', 'LuogoAccadimento']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provinces DataFrame cleaned and filtered for region: 'Sicilia'.\n"
     ]
    }
   ],
   "source": [
    "def clean_and_filter_provinces(df: pd.DataFrame, region_to_keep: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters the provinces DataFrame.\n",
    "\n",
    "    This function removes unnecessary administrative columns and then filters the\n",
    "    DataFrame to keep only the rows corresponding to a specific region.\n",
    "\n",
    "    Args:\n",
    "        df: The raw pandas DataFrame of provinces.\n",
    "        region_to_keep: The name of the region to keep in the final dataset (e.g., \"Sicilia\").\n",
    "\n",
    "    Returns:\n",
    "        A new, cleaned, and filtered pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Define all columns to be dropped in a single list for clarity.\n",
    "    columns_to_drop = [\n",
    "        \"CodCittaMetropolitana\", \"CodRegione\", \"CodMacroregione\", \n",
    "        \"DescrMacroregione\", \"CodNazione\", \"DescrNazione\", \n",
    "        \"DataInizioValidita\", \"DataFineValidita\"\n",
    "    ]\n",
    "    \n",
    "    # --- The \"Pandas-Idiomatic\" Way: Chaining Operations ---\n",
    "    # We first drop the columns, then apply the filter directly.\n",
    "    # This is much more readable and efficient.\n",
    "    \n",
    "    # Using Boolean Indexing (most common method)\n",
    "    filtered_df = df.drop(columns=columns_to_drop)\n",
    "    # This line reads like English: \"keep rows where DescrRegione is Sicilia\"\n",
    "    filtered_df = filtered_df[filtered_df['DescrRegione'] == region_to_keep].copy()\n",
    "\n",
    "    # Alternative using .query() method (often even more readable)\n",
    "    # cleaned_df = (\n",
    "    #     df.drop(columns=columns_to_drop)\n",
    "    #       .query(f\"DescrRegione == '{region_to_keep}'\")\n",
    "    #       .copy()\n",
    "    # )\n",
    "    \n",
    "    print(f\"Provinces DataFrame cleaned and filtered for region: '{region_to_keep}'.\")\n",
    "    return filtered_df\n",
    "\n",
    "# --- How to use the function ---\n",
    "# We use the 'provinces' DataFrame we loaded earlier\n",
    "sicilian_provinces = clean_and_filter_provinces(df=provinces, region_to_keep=\"Sicilia\")\n",
    "\n",
    "# print(sicilian_provinces.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame arricchito con successo. Ecco un'anteprima:\n",
      "  DataAccadimento DataMorte Genere  Eta ConSenzaMezzoTrasporto  \\\n",
      "0      2017-01-01       NaN      M   22                      N   \n",
      "1      2017-01-01       NaN      F   64                      N   \n",
      "2      2017-01-02       NaN      M   37                      N   \n",
      "3      2017-01-02       NaN      F   55                      N   \n",
      "4      2017-01-02       NaN      M   34                      N   \n",
      "\n",
      "   IdentificativoCaso  GiorniIndennizzati  year CodSiglaProvincia  \\\n",
      "0            20385143                  48  2017                TP   \n",
      "1            20403929                  15  2017                TP   \n",
      "2            20423242                   0  2017                TP   \n",
      "3            20438063                   1  2017                TP   \n",
      "4            20388488                   0  2017                TP   \n",
      "\n",
      "  LuogoAccadimento DescrRegione DefinizioneAmministrativa     Indennizzo  \\\n",
      "0          Trapani      Sicilia                  Positivo  In temporanea   \n",
      "1          Trapani      Sicilia                  Positivo  In temporanea   \n",
      "2          Trapani      Sicilia                  Negativo        Nessuno   \n",
      "3          Trapani      Sicilia                  Positivo  In temporanea   \n",
      "4          Trapani      Sicilia                  Negativo        Nessuno   \n",
      "\n",
      "  DecisioneIstruttoriaEsitoMortale LuogoNascita FlagAppartenenzaUE  \\\n",
      "0                  Non Applicabile       ITALIA                  S   \n",
      "1                  Non Applicabile       ITALIA                  S   \n",
      "2                  Non Applicabile       ITALIA                  S   \n",
      "3                  Non Applicabile       ITALIA                  S   \n",
      "4                  Non Applicabile       ITALIA                  S   \n",
      "\n",
      "  DescrGruppoPaese DataIngressoUE DataUscitaUE  \n",
      "0           Italia     01/07/1999   31/12/9999  \n",
      "1           Italia     01/07/1999   31/12/9999  \n",
      "2           Italia     01/07/1999   31/12/9999  \n",
      "3           Italia     01/07/1999   31/12/9999  \n",
      "4           Italia     01/07/1999   31/12/9999  \n"
     ]
    }
   ],
   "source": [
    "def enrich_incidents_data(main_df: pd.DataFrame, lookup_dfs: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enriches the main incidents DataFrame by merging it with multiple lookup tables.\n",
    "    ... (il resto della docstring va bene) ...\n",
    "    \"\"\"\n",
    "    enriched_df = main_df.copy()\n",
    "    \n",
    "    for original_col, (lookup_df, lookup_key, new_val_col) in lookup_dfs.items():\n",
    "        # Perform the merge\n",
    "        enriched_df = pd.merge(\n",
    "            enriched_df,\n",
    "            lookup_df,\n",
    "            left_on=original_col,\n",
    "            right_on=lookup_key,\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Drop the original code column and the now-redundant lookup key\n",
    "        # CORREZIONE: Aggiunto un controllo per non eliminare la colonna due volte se i nomi coincidono\n",
    "        columns_to_drop = {original_col, lookup_key}\n",
    "        enriched_df = enriched_df.drop(columns=list(columns_to_drop))\n",
    "        \n",
    "        # Rename the new descriptive column to the original column's name for consistency\n",
    "        enriched_df = enriched_df.rename(columns={new_val_col: original_col})\n",
    "\n",
    "    return enriched_df\n",
    "\n",
    "\n",
    "# --- How to use the function ---\n",
    "\n",
    "# Il dizionario 'merge_plan' è corretto\n",
    "merge_plan = {\n",
    "    'LuogoAccadimento': (sicilian_provinces, 'Provincia', 'DescrProvincia'),\n",
    "    'DefinizioneAmministrativa': (admin_definitions, 'DefinizioneAmministrativa', 'DescrDefinizioneAmministrativa'),\n",
    "    'Indennizzo': (compensation_types, 'TipologiaIndennizzo', 'DescrTipologiaIndennizzo'),\n",
    "    'DecisioneIstruttoriaEsitoMortale': (fatal_outcomes, 'DecisioneIstruttoriaEsitoMortale', 'DescrDecisioneIstruttoriaEsitoMortale'),\n",
    "    'LuogoNascita': (birthplaces, 'LuogoNascita', 'DescrNazioneNascita')\n",
    "}\n",
    "\n",
    "# --- ECCO LA CORREZIONE ---\n",
    "# Ho usato il nome corretto della funzione ('enrich_incidents_data')\n",
    "# e il nome corretto dell'argomento ('lookup_dfs')\n",
    "final_incidents_df = enrich_incidents_data(\n",
    "    main_df=sorted_incidents, \n",
    "    lookup_dfs=merge_plan\n",
    ")\n",
    "\n",
    "# Ora puoi visualizzare l'anteprima del DataFrame corretto\n",
    "print(\"DataFrame arricchito con successo. Ecco un'anteprima:\")\n",
    "print(final_incidents_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled NaNs in object column 'DataMorte' with 'Unknown'.\n",
      "Filled NaNs in object column 'DataIngressoUE' with 'Unknown'.\n",
      "Filled NaNs in object column 'DataUscitaUE' with 'Unknown'.\n",
      "\n",
      "Missing value handling and index reset complete.\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a strategic approach to handle missing values (NaNs).\n",
    "\n",
    "    This function handles missing values differently for numeric and object columns:\n",
    "    - Numeric columns: NaNs are filled with the median of the column.\n",
    "    - Object (categorical) columns: NaNs are filled with the string 'Unknown'.\n",
    "    This preserves data types and is a much more robust strategy.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame with missing values.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with NaNs handled strategically.\n",
    "    \"\"\"\n",
    "    handled_df = df.copy()\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for column in handled_df.columns:\n",
    "        # Check if the column has any missing values\n",
    "        if handled_df[column].isnull().any():\n",
    "            # STRATEGY FOR NUMERIC COLUMNS\n",
    "            if pd.api.types.is_numeric_dtype(handled_df[column]):\n",
    "                # Fill with the median, which is robust to outliers\n",
    "                median_value = handled_df[column].median()\n",
    "                handled_df[column] = handled_df[column].fillna(median_value)\n",
    "                print(f\"Filled NaNs in numeric column '{column}' with median value ({median_value}).\")\n",
    "                \n",
    "            # STRATEGY FOR CATEGORICAL/OBJECT COLUMNS\n",
    "            elif pd.api.types.is_object_dtype(handled_df[column]):\n",
    "                # Fill with a specific placeholder string\n",
    "                handled_df[column] = handled_df[column].fillna('Unknown')\n",
    "                print(f\"Filled NaNs in object column '{column}' with 'Unknown'.\")\n",
    "\n",
    "    # After handling NaNs, it's a good practice to reset the index.\n",
    "    handled_df = handled_df.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nMissing value handling and index reset complete.\")\n",
    "    return handled_df\n",
    "\n",
    "# --- How to use the function ---\n",
    "# Assuming 'final_incidents_df' is the output of the previous (merge) step\n",
    "final_incidents_df_handled = handle_missing_values(final_incidents_df)\n",
    "\n",
    "# Now, you can display the final, clean DataFrame\n",
    "# final_incidents_df_handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology built and saved successfully to 'workplace_safety_ontology.ttl'.\n",
      "Graph has 36 triples.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, XSD, OWL # Import OWL for advanced concepts\n",
    "\n",
    "def build_workplace_safety_ontology() -> Graph:\n",
    "    \"\"\"\n",
    "    Builds the RDFS ontology for modeling workplace incidents.\n",
    "\n",
    "    This ontology defines the core classes (Incident, Person, Location) and the\n",
    "    properties that connect them, following professional semantic web conventions.\n",
    "\n",
    "    Returns:\n",
    "        An rdflib.Graph object containing the complete ontology.\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "\n",
    "    # --- 1. Namespace Definitions (Professional Standard) ---\n",
    "    INLO = Namespace(\"http://example.com/workplace-safety/ontology#\")\n",
    "    \n",
    "    g.bind(\"inlo\", INLO)\n",
    "    g.bind(\"rdfs\", RDFS)\n",
    "    g.bind(\"owl\", OWL)\n",
    "\n",
    "    # --- 2. Class Definitions ---\n",
    "    \n",
    "    # The Person Class\n",
    "    g.add((INLO.Person, RDF.type, RDFS.Class))\n",
    "    g.add((INLO.Person, RDFS.label, Literal(\"Person\", lang=\"en\"))) # CORRETTO\n",
    "    g.add((INLO.Person, RDFS.label, Literal(\"Persona\", lang=\"it\"))) # CORRETTO\n",
    "    g.add((INLO.Person, RDFS.comment, Literal(\"Represents an individual involved in an incident.\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    # The Location Class\n",
    "    g.add((INLO.Location, RDF.type, RDFS.Class))\n",
    "    g.add((INLO.Location, RDFS.label, Literal(\"Location\", lang=\"en\"))) # CORRETTO\n",
    "    g.add((INLO.Location, RDFS.label, Literal(\"Luogo\", lang=\"it\"))) # CORRETTO\n",
    "    g.add((INLO.Location, RDFS.comment, Literal(\"A geographical location, such as a province.\", lang=\"en\"))) # CORRETTO\n",
    "    \n",
    "    # The Incident Class\n",
    "    g.add((INLO.Incident, RDF.type, RDFS.Class))\n",
    "    g.add((INLO.Incident, RDFS.label, Literal(\"Workplace Incident\", lang=\"en\"))) # CORRETTO\n",
    "    g.add((INLO.Incident, RDFS.label, Literal(\"Incidente sul Lavoro\", lang=\"it\"))) # CORRETTO\n",
    "    g.add((INLO.Incident, RDFS.comment, Literal(\"Represents a single workplace incident event.\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    # --- 3. Property Definitions ---\n",
    "    \n",
    "    # -- Person Properties --\n",
    "    g.add((INLO.hasGender, RDF.type, RDF.Property))\n",
    "    g.add((INLO.hasGender, RDFS.domain, INLO.Person))\n",
    "    g.add((INLO.hasGender, RDFS.range, XSD.string))\n",
    "    g.add((INLO.hasGender, RDFS.label, Literal(\"has gender\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    g.add((INLO.hasAge, RDF.type, RDF.Property))\n",
    "    g.add((INLO.hasAge, RDFS.domain, INLO.Person))\n",
    "    g.add((INLO.hasAge, RDFS.range, XSD.integer))\n",
    "    g.add((INLO.hasAge, RDFS.label, Literal(\"has age\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    # -- Incident Properties --\n",
    "    g.add((INLO.incidentDate, RDF.type, RDF.Property))\n",
    "    g.add((INLO.incidentDate, RDFS.domain, INLO.Incident))\n",
    "    g.add((INLO.incidentDate, RDFS.range, XSD.date))\n",
    "    g.add((INLO.incidentDate, RDFS.label, Literal(\"incident date\", lang=\"en\"))) # CORRETTO\n",
    "    \n",
    "    g.add((INLO.daysIndemnified, RDF.type, RDF.Property))\n",
    "    g.add((INLO.daysIndemnified, RDFS.domain, INLO.Incident))\n",
    "    g.add((INLO.daysIndemnified, RDFS.range, XSD.integer))\n",
    "    g.add((INLO.daysIndemnified, RDFS.label, Literal(\"days indemnified\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    # Object Properties (link classes to other classes)\n",
    "    g.add((INLO.involvesPerson, RDF.type, RDF.Property))\n",
    "    g.add((INLO.involvesPerson, RDFS.domain, INLO.Incident))\n",
    "    g.add((INLO.involvesPerson, RDFS.range, INLO.Person))\n",
    "    g.add((INLO.involvesPerson, RDFS.label, Literal(\"involves person\", lang=\"en\"))) # CORRETTO\n",
    "    \n",
    "    g.add((INLO.occurredAt, RDF.type, RDF.Property))\n",
    "    g.add((INLO.occurredAt, RDFS.domain, INLO.Incident))\n",
    "    g.add((INLO.occurredAt, RDFS.range, INLO.Location))\n",
    "    g.add((INLO.occurredAt, RDFS.label, Literal(\"occurred at\", lang=\"en\"))) # CORRETTO\n",
    "\n",
    "    return g\n",
    "\n",
    "# --- How to use the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    ontology_graph = build_workplace_safety_ontology()\n",
    "    \n",
    "    file_path = \"workplace_safety_ontology.ttl\"\n",
    "    ontology_graph.serialize(destination=file_path, format='turtle')\n",
    "    \n",
    "    print(f\"Ontology built and saved successfully to '{file_path}'.\")\n",
    "    print(f\"Graph has {len(ontology_graph)} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating graph from DataFrame rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 131748/131748 [00:57<00:00, 2277.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph population complete.\n",
      "\n",
      "Knowledge Graph successfully created and saved to 'knowledge_graph.ttl'.\n",
      "Final graph has 1185795 triples.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm # A library to create smart progress bars\n",
    "\n",
    "def populate_graph_from_dataframe(df: pd.DataFrame, ontology_graph: Graph) -> Graph:\n",
    "    \"\"\"\n",
    "    Populates the knowledge graph with instances from the DataFrame.\n",
    "\n",
    "    This function iterates through the DataFrame efficiently using itertuples()\n",
    "    and populates the graph according to the defined ontology.\n",
    "\n",
    "    Args:\n",
    "        df: The cleaned and enriched pandas DataFrame of incidents.\n",
    "        ontology_graph: The graph containing the ontology definitions.\n",
    "\n",
    "    Returns:\n",
    "        The populated RDF graph.\n",
    "    \"\"\"\n",
    "    # Work on a copy of the graph to avoid modifying the original ontology object\n",
    "    populated_g = ontology_graph\n",
    "\n",
    "    # Define namespaces (we use the ones from the ontology)\n",
    "    INLO = Namespace(\"http://example.com/workplace-safety/ontology#\")\n",
    "    RESOURCE = Namespace(\"http://example.com/workplace-safety/resource/\")\n",
    "    DBR = Namespace(\"http://dbpedia.org/resource/\")\n",
    "\n",
    "    # Bind prefixes for cleaner output\n",
    "    populated_g.bind(\"inlo\", INLO)\n",
    "    populated_g.bind(\"resource\", RESOURCE)\n",
    "    populated_g.bind(\"dbr\", DBR)\n",
    "\n",
    "    # Use df.itertuples() for a highly efficient iteration\n",
    "    # tqdm adds a helpful progress bar for long operations\n",
    "    print(\"Populating graph from DataFrame rows...\")\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        # --- 1. Create URIs for the main instances ---\n",
    "        # We use the robust create_uri function defined earlier\n",
    "        incident_uri = create_uri(RESOURCE, f\"incident_{row.IdentificativoCaso}\")\n",
    "        person_uri = create_uri(RESOURCE, f\"person_{row.Index}\") # Use the row index for a unique person ID\n",
    "        location_uri = create_uri(RESOURCE, row.LuogoAccadimento)\n",
    "\n",
    "        # --- 2. Add Triples for the Incident ---\n",
    "        populated_g.add((incident_uri, RDF.type, INLO.Incident))\n",
    "        populated_g.add((incident_uri, RDFS.label, Literal(f\"Incident involving person {row.Index}\", lang=\"en\")))\n",
    "        \n",
    "        # Add data properties, checking for valid data before adding\n",
    "        if pd.notna(row.DataAccadimento):\n",
    "            # Assuming DataAccadimento is already a datetime object from previous steps\n",
    "            # We format it to the correct XSD date format\n",
    "            date_literal = Literal(row.DataAccadimento.date().isoformat(), datatype=XSD.date)\n",
    "            populated_g.add((incident_uri, INLO.incidentDate, date_literal))\n",
    "\n",
    "        if pd.notna(row.GiorniIndennizzati):\n",
    "            populated_g.add((incident_uri, INLO.daysIndemnified, Literal(int(row.GiorniIndennizzati), datatype=XSD.integer)))\n",
    "\n",
    "        # --- 3. Add Triples for the Person ---\n",
    "        populated_g.add((person_uri, RDF.type, INLO.Person))\n",
    "        if pd.notna(row.Genere):\n",
    "             populated_g.add((person_uri, INLO.hasGender, Literal(row.Genere, datatype=XSD.string)))\n",
    "        if pd.notna(row.Eta):\n",
    "            populated_g.add((person_uri, INLO.hasAge, Literal(int(row.Eta), datatype=XSD.integer)))\n",
    "\n",
    "        # --- 4. Add Triples for the Location ---\n",
    "        populated_g.add((location_uri, RDF.type, INLO.Location))\n",
    "        populated_g.add((location_uri, RDFS.label, Literal(row.LuogoAccadimento, lang=\"it\")))\n",
    "        \n",
    "        # --- 5. Link Instances Together (Object Properties) ---\n",
    "        populated_g.add((incident_uri, INLO.involvesPerson, person_uri))\n",
    "        populated_g.add((incident_uri, INLO.occurredAt, location_uri))\n",
    "        \n",
    "        # --- 6. Link to External Data (DBpedia) - The Scalable Way ---\n",
    "        # Dynamically create the DBpedia URI from the location name\n",
    "        dbpedia_location_uri = create_uri(DBR, row.LuogoAccadimento)\n",
    "        populated_g.add((location_uri, OWL.sameAs, dbpedia_location_uri))\n",
    "\n",
    "    print(\"Graph population complete.\")\n",
    "    return populated_g\n",
    "\n",
    "# --- How to use the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. First, build the empty ontology structure\n",
    "    ontology = build_workplace_safety_ontology()\n",
    "    \n",
    "    # 2. Then, populate it using our cleaned DataFrame\n",
    "    # Note: Use the final, fully cleaned DataFrame from the previous steps\n",
    "    final_graph = populate_graph_from_dataframe(\n",
    "        df=final_incidents_df_handled, \n",
    "        ontology_graph=ontology\n",
    "    )\n",
    "    \n",
    "    # 3. Serialize the final graph to a relative path\n",
    "    output_path = \"knowledge_graph.ttl\"\n",
    "    final_graph.serialize(destination=output_path, format='turtle')\n",
    "    \n",
    "    print(f\"\\nKnowledge Graph successfully created and saved to '{output_path}'.\")\n",
    "    print(f\"Final graph has {len(final_graph)} triples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating graph from DataFrame rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131748/131748 [00:43<00:00, 3017.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph population complete.\n",
      "\n",
      "Knowledge Graph creato e salvato con successo in 'workplace_safety_knowledge_graph.ttl'.\n",
      "Il grafo finale contiene 1185795 triple.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "knowledge_graph = populate_graph_from_dataframe(\n",
    "    df=final_incidents_df, \n",
    "    ontology_graph=ontology\n",
    ")\n",
    "\n",
    "output_filename = \"workplace_safety_knowledge_graph.ttl\"\n",
    "knowledge_graph.serialize(destination=output_filename, format='turtle')\n",
    "\n",
    "print(f\"\\nKnowledge Graph creato e salvato con successo in '{output_filename}'.\")\n",
    "print(f\"Il grafo finale contiene {len(knowledge_graph)} triple.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "    SELECT ?incident ?date\n",
      "    WHERE {\n",
      "        ?incident a inlo:Incident ;\n",
      "                  inlo:occurredAt ?location ;\n",
      "                  inlo:incidentDate ?date .\n",
      "\n",
      "        ?location rdfs:label \"Catania\"@it .\n",
      "    }\n",
      "    ORDER BY ?date\n",
      "    LIMIT 10\n",
      "-----------------------\n",
      "Query found 10 results. Showing up to 10:\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20401491'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20428506'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20431809'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20395310'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20431318'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20403502'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20416822'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20385121'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20391155'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20393901'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "... (and 0 more)\n",
      "-----------------------\n",
      "\n",
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "    SELECT ?province (COUNT(?incident) AS ?incidentCount)\n",
      "    WHERE {\n",
      "        ?incident a inlo:Incident ;\n",
      "                  inlo:occurredAt ?location .\n",
      "\n",
      "        ?location rdfs:label ?province .\n",
      "    }\n",
      "    GROUP BY ?province\n",
      "    ORDER BY DESC(?incidentCount)\n",
      "-----------------------\n",
      "Query found 9 results. Showing up to 10:\n",
      "{'province': rdflib.term.Literal('Catania', lang='it'), 'incidentCount': rdflib.term.Literal('33021', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Palermo', lang='it'), 'incidentCount': rdflib.term.Literal('29016', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Messina', lang='it'), 'incidentCount': rdflib.term.Literal('16086', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Ragusa', lang='it'), 'incidentCount': rdflib.term.Literal('12361', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Siracusa', lang='it'), 'incidentCount': rdflib.term.Literal('11134', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Trapani', lang='it'), 'incidentCount': rdflib.term.Literal('10636', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Agrigento', lang='it'), 'incidentCount': rdflib.term.Literal('8719', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Caltanissetta', lang='it'), 'incidentCount': rdflib.term.Literal('6244', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'province': rdflib.term.Literal('Enna', lang='it'), 'incidentCount': rdflib.term.Literal('4531', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The import for rdflib should be at the top of the script.\n",
    "\n",
    "def execute_sparql_query(graph: Graph, query_string: str):\n",
    "    \"\"\"\n",
    "    Executes a SPARQL query on a local rdflib graph and prints the results.\n",
    "\n",
    "    Args:\n",
    "        graph: The rdflib.Graph object to query.\n",
    "        query_string: The SPARQL query to be executed.\n",
    "    \"\"\"\n",
    "    print(\"--- Executing Query ---\")\n",
    "    print(query_string.strip())\n",
    "    print(\"-----------------------\")\n",
    "    \n",
    "    # We use the graph's native .query() method, which is simple and efficient.\n",
    "    results = graph.query(query_string)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Query returned no results.\")\n",
    "        return\n",
    "\n",
    "    # The query result is an iterable object, so we can loop through it.\n",
    "    print(f\"Query found {len(results)} results. Showing up to 10:\")\n",
    "    for i, row in enumerate(results):\n",
    "        # row.asdict() conveniently converts the result into a dictionary.\n",
    "        print(row.asdict())\n",
    "        if i == 9: # Limit the printout for readability.\n",
    "            print(f\"... (and {len(results) - 10} more)\")\n",
    "            break\n",
    "            \n",
    "    print(\"-----------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Example of How to Use the Function ---\n",
    "# We use the 'knowledge_graph' object, which must already be created and populated in a previous cell.\n",
    "# There is NO NEED to save and re-parse the file from disk.\n",
    "\n",
    "# --- Example Query 1: Find all incidents that occurred in Catania ---\n",
    "query_incidents_in_catania = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?incident ?date\n",
    "    WHERE {\n",
    "        ?incident a inlo:Incident ;\n",
    "                  inlo:occurredAt ?location ;\n",
    "                  inlo:incidentDate ?date .\n",
    "        \n",
    "        ?location rdfs:label \"Catania\"@it .\n",
    "    }\n",
    "    ORDER BY ?date\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "execute_sparql_query(knowledge_graph, query_incidents_in_catania)\n",
    "\n",
    "\n",
    "# --- Example Query 2: Count the number of incidents per province ---\n",
    "query_count_by_province = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?province (COUNT(?incident) AS ?incidentCount)\n",
    "    WHERE {\n",
    "        ?incident a inlo:Incident ;\n",
    "                  inlo:occurredAt ?location .\n",
    "        \n",
    "        ?location rdfs:label ?province .\n",
    "    }\n",
    "    GROUP BY ?province\n",
    "    ORDER BY DESC(?incidentCount)\n",
    "\"\"\"\n",
    "execute_sparql_query(knowledge_graph, query_count_by_province)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "\n",
      "    SELECT ?person ?birthPlace\n",
      "    WHERE {\n",
      "        ?person a inlo:Person ;\n",
      "                inlo:hasBirthPlace ?birthPlace .\n",
      "\n",
      "        # This FILTER is more robust:\n",
      "        # 1. It converts the birthplace to lowercase before comparing.\n",
      "        # 2. It checks if the birthplace string contains \"italia\".\n",
      "        FILTER(CONTAINS(LCASE(?birthPlace), \"italia\"))\n",
      "    }\n",
      "    LIMIT 10\n",
      "-----------------------\n",
      "Query returned no results.\n"
     ]
    }
   ],
   "source": [
    "# --- Query 1: Find all people born in Italy ---\n",
    "# This query demonstrates a robust, case-insensitive filter.\n",
    "\n",
    "query_people_born_in_italy = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "\n",
    "    SELECT ?person ?birthPlace\n",
    "    WHERE {\n",
    "        ?person a inlo:Person ;\n",
    "                inlo:hasBirthPlace ?birthPlace .\n",
    "        \n",
    "        # This FILTER is more robust:\n",
    "        # 1. It converts the birthplace to lowercase before comparing.\n",
    "        # 2. It checks if the birthplace string contains \"italia\".\n",
    "        FILTER(CONTAINS(LCASE(?birthPlace), \"italia\"))\n",
    "    }\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# --- Execute the query using our standard function ---\n",
    "# We use the 'knowledge_graph' object that is already in memory.\n",
    "execute_sparql_query(knowledge_graph, query_people_born_in_italy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "    SELECT ?incident ?date\n",
      "    WHERE {\n",
      "        # 1. First, find the entity (?location) that is a Location and has the Italian label \"Palermo\".\n",
      "        ?location a inlo:Location ;\n",
      "                  rdfs:label \"Palermo\"@it .\n",
      "\n",
      "        # 2. Then, find any ?incident that is linked to that ?location\n",
      "        #    via the 'occurredAt' property.\n",
      "        ?incident a inlo:Incident ;\n",
      "                  inlo:occurredAt ?location ;\n",
      "                  inlo:incidentDate ?date .\n",
      "    }\n",
      "    ORDER BY ?date\n",
      "    LIMIT 10\n",
      "-----------------------\n",
      "Query found 10 results. Showing up to 10:\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20371223'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20384980'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20367146'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20435117'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20429223'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20429199'), 'date': rdflib.term.Literal('2017-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20391707'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20412292'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20388079'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "{'incident': rdflib.term.URIRef('http://example.com/workplace-safety/resource/incident_20646969'), 'date': rdflib.term.Literal('2017-01-02', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#date'))}\n",
      "... (and 0 more)\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Query 2: Find all incidents that occurred in Palermo ---\n",
    "# This query shows how to traverse relationships in the graph.\n",
    "\n",
    "query_incidents_in_palermo = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?incident ?date\n",
    "    WHERE {\n",
    "        # 1. First, find the entity (?location) that is a Location and has the Italian label \"Palermo\".\n",
    "        ?location a inlo:Location ;\n",
    "                  rdfs:label \"Palermo\"@it .\n",
    "        \n",
    "        # 2. Then, find any ?incident that is linked to that ?location\n",
    "        #    via the 'occurredAt' property.\n",
    "        ?incident a inlo:Incident ;\n",
    "                  inlo:occurredAt ?location ;\n",
    "                  inlo:incidentDate ?date .\n",
    "    }\n",
    "    ORDER BY ?date\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# --- Execute the query using our standard function ---\n",
    "execute_sparql_query(knowledge_graph, query_incidents_in_palermo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "    SELECT ?provinceLabel (COUNT(?incident) AS ?incidentCount)\n",
      "    WHERE {\n",
      "        # 1. Find all incidents.\n",
      "        ?incident a inlo:Incident .\n",
      "\n",
      "        # 2. Follow the 'occurredAt' link from the incident to its location.\n",
      "        ?incident inlo:occurredAt ?location .\n",
      "\n",
      "        # 3. Get the Italian label from that location.\n",
      "        ?location rdfs:label ?provinceLabel .\n",
      "    }\n",
      "    GROUP BY ?provinceLabel\n",
      "    ORDER BY DESC(?incidentCount)\n",
      "-----------------------\n",
      "Query found 9 results. Showing up to 10:\n",
      "{'provinceLabel': rdflib.term.Literal('Catania', lang='it'), 'incidentCount': rdflib.term.Literal('33021', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Palermo', lang='it'), 'incidentCount': rdflib.term.Literal('29016', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Messina', lang='it'), 'incidentCount': rdflib.term.Literal('16086', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Ragusa', lang='it'), 'incidentCount': rdflib.term.Literal('12361', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Siracusa', lang='it'), 'incidentCount': rdflib.term.Literal('11134', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Trapani', lang='it'), 'incidentCount': rdflib.term.Literal('10636', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Agrigento', lang='it'), 'incidentCount': rdflib.term.Literal('8719', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Caltanissetta', lang='it'), 'incidentCount': rdflib.term.Literal('6244', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'provinceLabel': rdflib.term.Literal('Enna', lang='it'), 'incidentCount': rdflib.term.Literal('4531', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Query 3: Count incidents per province and order them ---\n",
    "# This query demonstrates aggregation (GROUP BY) to find the provinces\n",
    "# with the highest number of incidents.\n",
    "\n",
    "query_count_by_province = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "    SELECT ?provinceLabel (COUNT(?incident) AS ?incidentCount)\n",
    "    WHERE {\n",
    "        # 1. Find all incidents.\n",
    "        ?incident a inlo:Incident .\n",
    "        \n",
    "        # 2. Follow the 'occurredAt' link from the incident to its location.\n",
    "        ?incident inlo:occurredAt ?location .\n",
    "        \n",
    "        # 3. Get the Italian label from that location.\n",
    "        ?location rdfs:label ?provinceLabel .\n",
    "    }\n",
    "    GROUP BY ?provinceLabel\n",
    "    ORDER BY DESC(?incidentCount)\n",
    "\"\"\"\n",
    "\n",
    "# --- Execute the query using our standard function ---\n",
    "execute_sparql_query(knowledge_graph, query_count_by_province)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing Query ---\n",
      "PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
      "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "\n",
      "    SELECT ?year (COUNT(?incident) AS ?incidentCount)\n",
      "    WHERE {\n",
      "        # 1. Find all incidents that have an incidentDate property.\n",
      "        ?incident a inlo:Incident ;\n",
      "                  inlo:incidentDate ?date .\n",
      "\n",
      "        # 2. Use the YEAR() function to extract the year part from the date literal.\n",
      "        BIND(YEAR(?date) AS ?year)\n",
      "    }\n",
      "    GROUP BY ?year\n",
      "    ORDER BY ?year\n",
      "-----------------------\n",
      "Query found 5 results. Showing up to 10:\n",
      "{'year': rdflib.term.Literal('2017', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), 'incidentCount': rdflib.term.Literal('28763', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'year': rdflib.term.Literal('2018', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), 'incidentCount': rdflib.term.Literal('28251', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'year': rdflib.term.Literal('2019', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), 'incidentCount': rdflib.term.Literal('27929', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'year': rdflib.term.Literal('2020', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), 'incidentCount': rdflib.term.Literal('22662', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "{'year': rdflib.term.Literal('2021', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), 'incidentCount': rdflib.term.Literal('24143', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))}\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Query 4: Count incidents per year to identify trends ---\n",
    "# This query demonstrates how to use SPARQL functions (like YEAR())\n",
    "# to perform more advanced temporal analysis.\n",
    "\n",
    "query_count_by_year = \"\"\"\n",
    "    PREFIX inlo: <http://example.com/workplace-safety/ontology#>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "    SELECT ?year (COUNT(?incident) AS ?incidentCount)\n",
    "    WHERE {\n",
    "        # 1. Find all incidents that have an incidentDate property.\n",
    "        ?incident a inlo:Incident ;\n",
    "                  inlo:incidentDate ?date .\n",
    "        \n",
    "        # 2. Use the YEAR() function to extract the year part from the date literal.\n",
    "        BIND(YEAR(?date) AS ?year)\n",
    "    }\n",
    "    GROUP BY ?year\n",
    "    ORDER BY ?year\n",
    "\"\"\"\n",
    "\n",
    "# --- Execute the query using our standard function ---\n",
    "execute_sparql_query(knowledge_graph, query_count_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     51\u001b[39m     fig_pie.show()\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# --- Example of How to Use the Function ---\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Call this function at the very end of your script,\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# after the DataFrame is fully cleaned and processed.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m generate_visualizations(\u001b[43mfinal_df\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# The import for plotly.express should be at the top of the script\n",
    "# with the other imports.\n",
    "\n",
    "def generate_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates and displays key visualizations from the final DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The final, cleaned, and enriched pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "    # It's a best practice to first prepare the data for the plot.\n",
    "    # We'll use English column names for the plots.\n",
    "    # Let's assume the final df has columns: 'LuogoAccadimento', 'Genere'\n",
    "    # We can rename them for plotting if needed, or use them directly if already renamed.\n",
    "    \n",
    "    # --- Visualization 1: Bar Chart of Incidents per Province ---\n",
    "    # A bar chart is better than a histogram for categorical data.\n",
    "    # Sorting the values makes the chart much more insightful.\n",
    "    \n",
    "    province_counts = df['LuogoAccadimento'].value_counts().reset_index()\n",
    "    province_counts.columns = ['Province', 'Incident Count']\n",
    "    \n",
    "    fig_bar = px.bar(\n",
    "        province_counts.sort_values('Incident Count', ascending=False),\n",
    "        x='Province',\n",
    "        y='Incident Count',\n",
    "        title=\"Total Workplace Incidents by Province (2017-2021)\",\n",
    "        labels={'Province': 'Province in Sicily', 'Incident Count': 'Total Number of Incidents'},\n",
    "        text_auto=True,      # Displays the count on top of each bar\n",
    "        template='plotly_white' # A clean, professional theme\n",
    "    )\n",
    "    fig_bar.update_traces(marker_color='royalblue') # A more professional color\n",
    "    fig_bar.show()\n",
    "\n",
    "    # --- Visualization 2: Bar Chart for Gender Distribution ---\n",
    "    # While a pie chart works, bar charts are often easier to interpret accurately.\n",
    "    gender_counts = df['Genere'].value_counts().reset_index()\n",
    "    gender_counts.columns = ['Gender', 'Count']\n",
    "    \n",
    "    fig_pie = px.pie(\n",
    "        gender_counts, \n",
    "        names='Gender', \n",
    "        values='Count',\n",
    "        title=\"Distribution of Incidents by Gender\",\n",
    "        hole=0.3, # A donut chart is often more visually appealing\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig_pie.update_traces(textinfo='percent+label', marker_colors=['skyblue', 'salmon'])\n",
    "    fig_pie.show()\n",
    "\n",
    "# --- Example of How to Use the Function ---\n",
    "# Call this function at the very end of your script,\n",
    "# after the DataFrame is fully cleaned and processed.\n",
    "generate_visualizations(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prima crea una cartella 'images' nella directory del tuo progetto!\n",
    "fig_bar.write_image(\"images/incidents_by_province.png\")\n",
    "fig_pie.write_image(\"images/incidents_by_gender.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
